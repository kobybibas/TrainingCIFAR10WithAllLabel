{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result summary notebook\n",
    "This notebook explains what experimnet was done on each of the output and organizes the products in list so it can be used analyze_resulse.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NML on Cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the PNML on cifar10 dataset. Strating from pretrained resnet20 from https://github.com/akamaster/pytorch_resnet_cifar10. \n",
    "For each label, the model is trained for 10 epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training only 2 last layers for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_nml_2_layers = ['../output/TMP_results_20181025_092911/results_TMP_20181025_092911.json', # 000-099\n",
    "                      '../output/TMP_results_20181025_092956/results_TMP_20181025_092956.json', # 100-199\n",
    "                      '../output/TMP_results_20181025_093002/results_TMP_20181025_093002.json', # 200-299\n",
    "                      '../output/TMP_results_20181025_093009/results_TMP_20181025_093009.json', # 300-399\n",
    "                      '../output/NML_results_20181025_124946/results_NML_20181025_124946.json', # 400-499\n",
    "                      '../output/NML_results_20181025_125050/results_NML_20181025_125050.json', # 500-599\n",
    "                      '../output/NML_results_20181025_125116/results_NML_20181025_125116.json', # 600-699\n",
    "                      '../output/NML_results_20181025_125155/results_NML_20181025_125155.json', # 700-799\n",
    "                      '../output/NML_results_20181025_125239/results_NML_20181025_125239.json', # 800-899\n",
    "                      '../output/NML_results_20181025_125317/results_NML_20181025_125317.json', # 900-999\n",
    "                      '../output/NML_results_20181025_125338/results_NML_20181025_125338.json', # 1000-1099\n",
    "                      '../output/NML_results_20181025_125346/results_NML_20181025_125346.json'] # 1100-1199"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training only 5 last layers for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_nml_5_layers = ['../output/NML_results_20181028_115754/results_NML_20181028_115754.json']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training all the 7 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_nml_7_layers = ['../output/NML_results_20180820_084237/results_NML_20180820_084237.json', # 000-099\n",
    "                      '../output/NML_results_20180815_135021/results_NML_20180815_135021.json', # 100-199\n",
    "                      '../output/NML_results_20180815_134312/results_NML_20180815_134312.json', # 200-299\n",
    "                      '../output/NML_results_20180815_134318/results_NML_20180815_134318.json', # 300-399\n",
    "                      '../output/NML_results_20180815_134413/results_NML_20180815_134413.json', # 400-499\n",
    "                      '../output/NML_results_20180815_134420/results_NML_20180815_134420.json', # 500-599\n",
    "                      '../output/NML_results_20180815_134427/results_NML_20180815_134427.json', # 600-699\n",
    "                      '../output/NML_results_20180815_134449/results_NML_20180815_134449.json', # 700-799\n",
    "                      '../output/NML_results_20180815_134501/results_NML_20180815_134501.json', # 800-899\n",
    "                      '../output/NML_results_20180815_134527/results_NML_20180815_134527.json'] # 900-999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of disterbution\n",
    "Starting from pretrained resenet20 which was trained on cifar10. \n",
    "The test images are not from cifar10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noise Images. \n",
    "The test images are noise images. Each pixel generated from gaussin noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_noise = ['../output/OutOfDist_Noise_results_20181029_112825/results_OutOfDist_Noise_20181029_112825.json'] # 25-100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  SVHN\n",
    "Test images are from http://ufldl.stanford.edu/housenumbers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_svhn = ['../output/OutOfDist_SVHN_results_20181030_102907/results_OutOfDist_SVHN_20181030_102907.json']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random labels\n",
    "Training WideResnet Model with cifar10 dataset, however, the labels are assigned with some random probability.\n",
    "The code is based on https://github.com/pluskid/fitting-random-labels#command-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random probalility 0.0\n",
    "files_random_0 = ['../output/Random_Labels_results_20181012_101001/results_Random_Labels_20181012_101001.json'] # 0-99\n",
    "\n",
    "# Random probalility 0.01\n",
    "files_random_1 = ['../output/Random_Labels_results_20181012_101106/results_Random_Labels_20181012_101106.json'] # 0-99\n",
    "\n",
    "# Random probalility 0.03\n",
    "files_random_2 = ['../output/Random_Labels_results_20181012_101147/results_Random_Labels_20181012_101147.json'] # 0-99\n",
    "\n",
    "# Random probalility 0.1\n",
    "files_random_3 = ['../output/Random_Labels_results_20181012_101159/results_Random_Labels_20181012_101159.json'] # 0-99\n",
    "\n",
    "# Random probalility 0.3\n",
    "files_random_4 = ['../output/Random_Labels_results_20181012_101222/results_Random_Labels_20181012_101222.json'] # 0-99\n",
    "\n",
    "# Random probalility 0.5\n",
    "files_random_5 = ['../output/Random_Labels_results_20181012_101238/results_Random_Labels_20181012_101238.json'] # 0-99\n",
    "\n",
    "# Random probalility 0.6\n",
    "files_random_6 = ['../output/Random_Labels_results_20181012_101353/results_Random_Labels_20181012_101353.json'] # 0-99\n",
    "\n",
    "# Random probalility 1.0\n",
    "files_random_7 = ['../output/Random_Labels_results_20181012_101406/results_Random_Labels_20181012_101406.json'] # 0-99\n",
    "\n",
    "files_random = [files_random_0, files_random_1, files_random_2, files_random_3,\n",
    "                files_random_4, files_random_5, files_random_6, files_random_7]\n",
    "random_prob = [0.0, 0.01, 0.03, 0.1, 0.3, 0.5, 0.6, 1.0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERM\n",
    "Running the pretrained model in order to create file in the \"NML\" style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_erm = ['../output/ERM_results_20180813_204946/results_ERM_20180813_204946.json'] # 0-10,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training set Subset\n",
    "Training the base model with only subset of the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fix number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set size 100\n",
    "files_subset_fix_epochs_0 = ['../output/NML_trainset_subset_results_20180922_120302/results_NML_trainset_subset_20180922_120302.json'] # 0-99\n",
    "\n",
    "# training set size 300\n",
    "files_subset_fix_epochs_1 = ['../output/NML_trainset_subset_results_20180922_120325/results_NML_trainset_subset_20180922_120325.json'] # 0-99\n",
    "\n",
    "# training set size 1,000\n",
    "files_subset_fix_epochs_2 = ['../output/NML_trainset_subset_results_20180922_120401/results_NML_trainset_subset_20180922_120401.json'] # 0-99\n",
    "\n",
    "# training set size 3,000\n",
    "files_subset_fix_epochs_3 = ['../output/NML_trainset_subset_results_20180922_120758/results_NML_trainset_subset_20180922_120758.json'] # 0-99\n",
    "\n",
    "# training set size 10,000\n",
    "files_subset_fix_epochs_4 = ['../output/NML_trainset_subset_results_20180922_120524/results_NML_trainset_subset_20180922_120524.json'] # 0-99\n",
    "\n",
    "# training set size 30,000\n",
    "files_subset_fix_epochs_5 = ['../output/NML_trainset_subset_results_20180922_120521/results_NML_trainset_subset_20180922_120521.json'] # 0-99\n",
    "\n",
    "# training set size 50,000\n",
    "files_subset_fix_epochs_6 = ['../output/NML_trainset_subset_results_20180922_120421/results_NML_trainset_subset_20180922_120421.json'] # 0-99\n",
    "\n",
    "# Combine\n",
    "files_subset_fix_epochs = [files_subset_fix_epochs_0, files_subset_fix_epochs_1, files_subset_fix_epochs_2,\n",
    "                           files_subset_fix_epochs_3, files_subset_fix_epochs_4, files_subset_fix_epochs_5,\n",
    "                           files_subset_fix_epochs_6]\n",
    "subset_size_fix_epochs = [100, 300, 1000, 3000, 10000, 30000, 50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fix sgd update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set size 100\n",
    "files_subset_fix_sgd_0 = ['../output/NML_trainset_subset_results_20180930_090805/results_NML_trainset_subset_20180930_090805.json'] # 0-99\n",
    "\n",
    "# training set size 500\n",
    "files_subset_fix_sgd_1 = ['../output/NML_trainset_subset_results_20180930_090815/results_NML_trainset_subset_20180930_090815.json'] # 0-99\n",
    "\n",
    "# training set size 1,000\n",
    "files_subset_fix_sgd_2 = ['../output/NML_trainset_subset_results_20180930_090837/results_NML_trainset_subset_20180930_090837.json'] # 0-99\n",
    "\n",
    "# training set size 5,000\n",
    "files_subset_fix_sgd_3 = ['../output/NML_trainset_subset_results_20180930_090840/results_NML_trainset_subset_20180930_090840.json'] # 0-99\n",
    "\n",
    "# training set size 10,000\n",
    "files_subset_fix_sgd_4 = ['../output/NML_trainset_subset_results_20180930_090905/results_NML_trainset_subset_20180930_090905.json'] # 0-99\n",
    "\n",
    "# training set size 12500\n",
    "files_subset_fix_sgd_5 = ['../output/NML_trainset_subset_results_20180930_090907/results_NML_trainset_subset_20180930_090907.json'] # 0-99\n",
    "\n",
    "# training set size 25,000\n",
    "files_subset_fix_sgd_6 = ['../output/NML_trainset_subset_results_20180930_090908/results_NML_trainset_subset_20180930_090908.json'] # 0-99\n",
    "\n",
    "# training set size 50,000\n",
    "files_subset_fix_sgd_7 = ['../output/NML_trainset_subset_results_20180930_090911/results_NML_trainset_subset_20180930_090911.json'] # 0-99\n",
    "\n",
    "# Combine\n",
    "files_subset_fix_epochs = [files_subset_fix_sgd_0, files_subset_fix_sgd_1, files_subset_fix_sgd_2,\n",
    "                           files_subset_fix_sgd_3, files_subset_fix_sgd_4, files_subset_fix_sgd_5,\n",
    "                           files_subset_fix_sgd_6, files_subset_fix_sgd_7]\n",
    "subset_size_fix_sgd = [100, 500, 1000, 5000, 10000, 12500, 25000, 50000]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pt36]",
   "language": "python",
   "name": "conda-env-pt36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
